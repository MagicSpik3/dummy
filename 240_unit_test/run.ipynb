# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.1
#   kernelspec:
#     display_name: survey-assist-utils-PWI-TvqZ-py3.12
#     language: python
#     name: python3
# ---

# %%
"""Runs to Title JsonProcessor."""
from typing import TypedDict

import pandas as pd
import toml
from IPython.display import Markdown, display
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path


# gcloud auth application-default login
from survey_assist_utils.evaluation.coder_alignment import (
    ColumnConfig,
    ConfusionMatrixConfig,
    LabelAccuracy,
    PlotConfig,
)
from survey_assist_utils.evaluation.preprocessor import JsonPreprocessor

# %% [markdown]
# # Survey Assist Evaluation Runner
#
# This script includes all the metrics required for the PPT presentation as follows:
#
# For security, the actual bucket name has been changed to "\<my-butket-name\>"
#
# 1) Observations in labelled set.
# 2) Variability across SIC sections.
# 3) Match of top CC vs top SA on unambiguously Codable - 2 digis, 5 digit.
# 4) Match of top CC vs any SA on unambiguously codable - 2 digis, 5 digit.
# 5) Match rate top CC vs any SA on all data - 2 digis, 5 digit.
# 6) Match rate any CC vs any SA on all data - 2 digis, 5 digit.
# 7) Jaccard's Score all CC vs all SA, all data - 2 digis, 5 digit.
#
# We will first process the JSON output from the original 2000 data batch run.
#
# This requires a setup toml file, in 'prepare_config.toml' in this directory.

# %% [markdown]
# ### Configuration
# We first load the configuration and set up the main evaluation cases.

# %%
with open("prepare_config.toml", encoding="utf-8") as file:
    config = toml.load(file)

# Define the metrics to run:
class EvaluationCase(TypedDict):
    """Represents a single Title case configuration for evaluating model behavior."""
    Title: str
    CCs: list[int]
    LLMs: list[int]
    Unambiguous: bool

evaluation_cases_main: list[EvaluationCase] = [
    {
        "Title": "Match of top CC vs top SA on unambiguously Codable",
        "CCs": [1], "LLMs": [1], "Unambiguous": True,
    },
    {
        "Title": "Match of top CC vs any SA on unambiguously codable",
        "CCs": [1], "LLMs": [5], "Unambiguous": True,
    },
    {
        "Title": "Match rate top CC vs any SA on all data",
        "CCs": [1], "LLMs": [5], "Unambiguous": False,
    },
    {
        "Title": "Match rate any CC vs any SA on all data",
        "CCs": [3], "LLMs": [5], "Unambiguous": False,
    },
    {
        "Title": "Jaccard's Score all CC vs all SA, all data",
        "CCs": [3], "LLMs": [5], "Unambiguous": False,
    },
]

# %% [markdown]
# ### Data Preparation
# This section processes the raw JSON output from the model, flattens it, and merges it with the annotated ground truth data. This only needs to be run once per model output file.

# %%
# Get a list of files to check:
preprocessor = JsonPreprocessor(config)
record_count = preprocessor.count_all_records()
print(f"Found {record_count} records to process in the specified JSON file(s).")

llm_processed_df = preprocessor.process_files()
print("LLM Processed DataFrame Shape:", llm_processed_df.shape)

# %%
# Take the output from the preparation script and make it the input to the merging:
config["paths"]["processed_csv_output"] = config["paths"]["analysis_csv"]
full_output_df = preprocessor.merge_eval_data(llm_processed_df)
print("Fully Merged DataFrame Shape:", full_output_df.shape)


# %% [markdown]
# ---
# ## 1) Observations from the Labelled Set
# This section provides a high-level overview of the codability of the ground truth dataset. It shows the proportion of records that are uncodeable, codeable to different digit levels, and ambiguous vs. unambiguous.

# %%
def display_group_membership_analysis(df: pd.DataFrame):
    """Calculates and displays a pie chart and summary of group memberships."""
    N = len(df)
    
    # Calculate counts for each category
    count_uncodeable = (df["num_answers"] == 0).sum()
    count_2_digits_only = df["Match_2_digits"].sum()
    count_5_digits_unambiguous = df["Unambiguous"].sum()
    count_5_digits_ambiguous = (df["Match_5_digits"] & ~df["Unambiguous"]).sum()
    count_3_digits_only = df["Match_3_digits"].sum()
    count_four_plus = (df["num_answers"] == 4).sum()
    
    group_counts = [
        count_uncodeable, count_four_plus, count_2_digits_only,
        count_3_digits_only, count_5_digits_unambiguous, count_5_digits_ambiguous
    ]
    
    group_labels = [
        "Uncodeable", "4+ Codes", "Codeable to 2 digits only",
        "Codeable to 3 digits only", "Codeable unambiguously to 5 digits",
        "Codeable ambiguously to 5 digits"
    ]
    
    total_categorised = sum(group_counts)
    count_other = N - total_categorised
    
    if count_other > 0:
        group_counts.append(count_other)
        group_labels.append("Other")

    # Create and display the pie chart
    plt.figure(figsize=(10, 10))
    plt.pie(
        group_counts,
        autopct=lambda p: f"{p:.1f}%" if p > 1 else "",
        startangle=90,
        colors=sns.color_palette("pastel", len(group_counts))
    )
    plt.title("Distribution of Group Membership in Labelled Data", fontsize=16)
    plt.legend(labels=[f"{label} ({count/N*100:.1f}%)" for label, count in zip(group_labels, group_counts)], loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
    plt.axis("equal")
    plt.show()
    plt.close()

    # Create and display the summary table
    percentages = [(count / N) * 100 for count in group_counts]
    summary_df = pd.DataFrame({
        "Category": group_labels,
        "Count": group_counts,
        "Percentage": percentages
    })
    display(summary_df)

display_group_membership_analysis(full_output_df)


# %% [markdown]
# ## 2) Variability Across SIC Sections
# This section shows the distribution of the top 2-digit SIC codes (Divisions) in the labelled dataset to identify any potential bias.

# %%
def plot_sic_division_histogram(df: pd.DataFrame):
    """Generates and displays a histogram for the SIC_Division column."""
    plt.figure(figsize=(12, 8))
    
    counts = df['SIC_Division'].value_counts(normalize=True).nlargest(10) * 100
    
    ax = sns.barplot(
        x=counts.index,
        y=counts.values,
        palette="viridis"
    )
    ax.set_title(f"Top 10 Most Frequent SIC Divisions (Total Records: {len(df)})")
    plt.ylabel("Percentage")
    plt.xlabel("SIC Division (2-Digit Code)")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()
    plt.close()

plot_sic_division_histogram(full_output_df)


# %% [markdown]
# ---
# ## 3-7) Core Evaluation Metrics
# This section runs the main evaluation cases defined in the configuration, calculating accuracy and Jaccard scores for different scenarios.

# %%
def all_results(df, evaluation_case):
    """Executes and displays the results for a list of evaluation cases."""
    # Set up standard column names
    model_label_cols = [f"candidate_{i}_sic_code" for i in range(1, 6)]
    model_score_cols = [f"candidate_{i}_likelihood" for i in range(1, 6)]
    clerical_label_cols = [f"sic_ind_occ{i}" for i in range(1, 4)]

    results_data = []

    if df is not None:
        for case in evaluation_case:
            display(Markdown(f"### {case['Title']}"))
            
            config_main = ColumnConfig(
                model_label_cols=model_label_cols[: case["LLMs"][0]],
                model_score_cols=model_score_cols[: case["LLMs"][0]],
                clerical_label_cols=clerical_label_cols[: case["CCs"][0]],
                id_col="unique_id",
                filter_unambiguous=case["Unambiguous"],
            )

            analyzer_main = LabelAccuracy(df=df, column_config=config_main)

            full_acc_stats = analyzer_main.get_accuracy(match_type="full", extended=True)
            digit_acc_stats = analyzer_main.get_accuracy(match_type="2-digit", extended=True)
            
            jaccard_full = analyzer_main.get_jaccard_similarity(match_type="full")
            jaccard_2_digit = analyzer_main.get_jaccard_similarity(match_type="2-digit")

            case_results = {
                "Title": case['Title'],
                "5-Digit Accuracy (%)": full_acc_stats.get('accuracy_percent', 0),
                "2-Digit Accuracy (%)": digit_acc_stats.get('accuracy_percent', 0),
                "5-Digit Jaccard": jaccard_full,
                "2-Digit Jaccard": jaccard_2_digit,
                "Records Analyzed": full_acc_stats.get('total_considered', 0)
            }
            results_data.append(case_results)

    results_df = pd.DataFrame(results_data)
    display(results_df.round(2))

# %%
# Run the main evaluation loop
all_results(full_output_df, evaluation_cases_main)

# %% [markdown]
# ### Process and Evaluate Other Merged Files
# The following loop processes any additional pre-merged files listed in the config.

# %%
for this_file in config["paths"].get("merged_file_list", []):
    display(Markdown(f"# Results for: {this_file.split('/')[-1]}"))
    file_df = pd.read_csv(this_file, dtype=str)
    all_results(file_df, evaluation_cases_main)

